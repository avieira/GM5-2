\part{Conditions d'optimalité}
\section{Introduction aux problèmes d'optimisation}
\subsection{Terminologie}
On considère un problème du type
\begin{equation}\label{optimGal}
\begin{aligned}
	\min\ & f_0(x)\\
	\text{t.q. } & f_i(x)\leq 0,& i=1,...,m\\
			& h_i(x)=0,& i=1,...,p 
\end{aligned}
\end{equation}
On appelle 
\begin{itemize}
	\item $\mathcal{U}_{ad}$ \textit{l'ensemble admissible} des points vérifiant les contraintes d'inégalité et d'égalité
	\item \textit{point admissible} un point de $\mathcal{U}_{ad}$
	\item \textit{valeure optimale} $p^*$ de \eqref{optimGal} : \[p^*=\inf\{f_0(x) | f_i(x)\leq 0, i=1,...,m, h_i(x)=0, i=1,...,p\}\]
	\item \textit{ensemble optimal} l'ensemble des \textit{points optimaux} $x^*$ réalisant l'infimum : \[X_{opt}=\{x^* | f_i(x^*)\leq 0, i=1,...,m, h_i(x^*)=0, i=1,...,p,\ f(x^*)=p^*\}\]
	\item \textit{$\epsilon$-suboptimal} un point admissible $x$ tel que $f_0(x)\leq p^*+\epsilon$
	\item un point $x$ \textit{locallement optimal} s'il existe $R>0$ tel que \[f_0(x)=\inf\{f_0(z) | f_i(z)\leq 0, i=1,...,m, h_i(z)=0, i=1,...,p,\ \|x-z\|<R\}\]
\end{itemize} 

\subsection{Quelques formes équivalentes}
\paragraph{Problème sous forme d'épigraphe}
On peut remettre le problème \eqref{optimGal} sous la forme :
\[\begin{aligned}
	\min\ & t\\
	\text{t.q. } & f_0(x)-t\leq 0\\
			&f_i(x)\leq 0,& i=1,...,m\\
			& h_i(x)=0,& i=1,...,p 
\end{aligned}\]

\paragraph{Problèmes différentiables avec contraintes linéaires : première approche des multiplicateurs}
Prenons le problème suivant où $f_0$ est Fréchet-différentiable :
\[\begin{aligned}
	\min\ & f_0(x)\\
	\text{t.q. } & Ax=b
\end{aligned}\]
Les conditions d'optimalité pour un $x$ admissibles sont :
	\[\langle \nabla f_0(x),y-x\rangle\geq 0\]
pour tout $y$ vérifiant $Ay=b$. Comme $x$ est admissible, tout $y$ admissible est de la forme $y=x+v$ avec $v\in$ ker$(A)$. On peut donc réécrire la condition d'optimalité comme :
	\[\langle \nabla f_0(x),v\rangle\geq 0,\ \forall v\in\text{ker}(A)\]
et comme $v$ est quelconque dans un espace vectoriel (rempalcer $v$ par $-v$) : 
	\[\nabla f_0(x)\perp\text{ker}(A)\]
Mais comme ker$(A)^\perp=$Im$(A^\intercal)$, on peut dire que $\nabla f_0(x)\in$Im$(A^\intercal)$, ou encore qu'il existe $\nu\in\mathbb{R}^p$ tel que :
	\[\nabla f_0(x)+A^\intercal \nu=0\]

\section{Dualité}
On s'intéresse aux problème sous forme standard \eqref{optimGal} (qui est appelé pour la suite problème primal) avec $x\in\mathbb{R}^n$. On note $\mathcal{U}_{ad}=\bigcap_{i=0}^m$ dom $f_i\cap \bigcap_{i=1}^p $ dom $h_i$, le domaine admissible, qu'on suppose non vide, et $p^*$ la valeur optimale. L(idée de la dualité est de former un coût augmenté : on définit $L:\mathcal{U}_{ad}\times\mathbb{R}^m\times\mathbb{R}^p\to\mathbb{R}$ par :
	\[L(x,\lambda,\nu)=f_0(x)+\sum_{i=1}^m \lambda_i f_i(x)+\sum_{i=1}^p \nu_i h_i(x)\]
et la fonction duale $g:\mathbb{R}^m\times\mathbb{R}^p\to\mathbb{R}$ par :
	\[g(\lambda,\mu)=\inf_{x\in\mathcal{U}_{ad}} L(x,\lambda,\nu)\]
Cette fonction duale duale donne une borne inferieure à la valeur $p^*$ : pour tout $\lambda\geq 0$ et tout $\nu$, \[g(\lambda,\nu)\leq p^*\]
Une question naturelle est de savoir quelle est la \textit{meilleure} borne inférieure qu'on puisse obtenir. Pour cela, on s'intéresse au problème :
\begin{equation}\label{maxDual}
	\max_{\lambda\in\mathbb{R}^m_+,\ \nu\in\mathbb{R}^p} g(\lambda,\nu)
\end{equation}
Ce problème, peu importe la forme de \eqref{optimGal}, admet un obejectif concave et un domaine convexe. La valeure optimale de ce problème, notée $d^*$, vérifie :
	\[d^*\leq p^*\]
On appelle cette propriété \textit{dualité faible}, et on appelle \textit{écart dual} la valeur $p^*-d^*$. \\
Sous certaines conditions, on vérifie que $d^*=p^*$ ; c'est ce qu'on appelle la \textit{dualité forte}. Une de ces conditions est la condition de Slater, présentée plus loin. 

\subsection{Conditions KKT}
Supposons pour le moment que les $f_i$ et $h_i$ sont différentiables. On note $x^*$ et $(\lambda^*,\nu^*)$ les minimiseurs des problèmes primal et dual respectivement. Comme $x^*$ minimise $L(x,\lambda^*,\nu^*)$ sur $x$, on a que le gradient s'annule en $x^*$, i.e. :
	\[\nabla f_0(x^*)+\sum_{i=1}^m \lambda_i^* \nabla f_i(x^*)+\sum_{i=1}^p \nu_i^*\nabla h_i(x^*)=0\]
On a donc que si on vérifie la dualité forte, alors tout point primal-dual optimal $(x^*,\lambda^*,\nu^*)$ doit vérifier :
\[\begin{aligned}
	f_i(x^*)\leq 0,&\ i=1,...,m\\
	h_i(x^*)=0,&\ i=1,...,p\\
	\lambda_i^*\geq 0,&\ i=1,...,m\\
	\lambda_i f_i(x^*)=0,&\ i=1,...,m\\
\nabla f_0(x^*)+\sum_{i=1}^m \lambda_i^* \nabla f_i(x^*)+\sum_{i=1}^p \nu_i^*\nabla h_i(x^*)=0
\end{aligned}\]
ce qu'on appelle les conditions KKT.

\section{Problèmes convexes}
\[\min_{u\in\mathcal{U}_{ad}} J(u)\]
$\mathcal{U}_{ad}\subset\mathbb{R}^n$ est l'ensemble (non vide) admissible.
On suppose $\mathcal{U}_{ad}$ fermé convexe, et $J$ convexe.

\Theo{}{Si $J$ est coercive ou si $\mathcal{U}_{ad}$ est borné, alors il existe un point de minimum.}

\subsection{Une condition nécessaire générale d'optimalité}
\Def{Cône tangent}{On dit que $d\in\mathbb{R}^n$ est une tangente à $X$ en $\bar{x}$ si $\exists x_k\to\bar{x}$ avec $(x_k\subset X$, $t_k\to 0$, $t_k>0$ tel que :
	\[\frac{x_k-\bar{x}}{t_k}\to d\]
L'ensemble de toutes les directions tangentes est appelé le cône tangent et est noté $T_{\bar{x}}X$.}

\Def{équivalente}{$d\in T_{\bar{x}}X$ si $\exists t_k>0$, $t_k\to 0$ et $\exists d_k\in X$, $d_k\to d$ tel que $\bar{x}+t_kd_k\in X$.}

\Propo{}{$T_{\bar{x}}X$ est un cône fermé. Il est convexe si $X$ est convexe.}

\Propo{}{Soient $X$ un ensemble convexe et $\bar{x}\in X$. Alors \[T_{\bar{x}}X=\overline{cone}(X-\bar{x})=\overline{\mathbb{R}_+(X-\bar{x})}\]}

\Def{}{Soient $X\subset\mathbb{R}^n$, $\bar{x}\in X$.\\
On dit que $p$ est une direction normale à $X$ en $\bar{x}$ si \[\langle p,d\rangle\leq 0\ \forall d\in T_{\bar{x}}X\]
L'ensemble des normales est appelé le cône normal, noté $\mathcal{N}_{\bar{x}}X$.}

\Rem{}{$\mathcal{N}_{\bar{x}}X=\left(T_{\bar{x}}X\right)^-=-\left(T_{\bar{x}}X\right)^*$\\
$\mathcal{N}_{\bar{x}}X$ est donc un cône convexe.}

\Theo{}{Soit $\mathcal{U}_{ad}$ un ensemble convexe fermé non vide, $J:\mathbb{R}^n\to\mathbb{R}$ une fonction convexe, et $\bar{u}\in\mathcal{U}_{ad}$. Les assertions suivantes sont équivalentes :
\begin{enumerate}
	\item $\bar{u}$ minimise $J$ sur $\mathcal{U}_{ad}$
	\item $J'(\bar{u},u-\bar{u})\geq 0$ $\forall u\in \mathcal{U}_{ad}$
	\item $J'(\bar{u},d)\geq 0$ $\forall d\in T_{\bar{u}}\mathcal{U}_{ad}$
	\item $0\in\partial J(\bar{u}) + \mathcal{N}_{\bar{u}}\mathcal{U}_{ad}$.
\end{enumerate}}

\subsection{Cas où les contraintes sont explicites}
\[\mathcal{U}_{ad}=\left\{\begin{array}{c l r}
	u; & \langle a_i, u\rangle = b_i & i=1,...,m\\
	   & c_j(u)\leq 0 		 & j=1,...,p
\end{array}\right\} \]

$a_i\in\mathbb{R}^n$, $b_i\in\mathbb{R}$, $c_j:\mathbb{R}^n\to\mathbb{R}$ convexe.\\
$\mathcal{U}_{ad}$ est convexe.\\
On note \[\begin{array}{l c c c}
A : & \mathbb{R}^n &\to& \mathbb{R}^m\\
    &       u    &\mapsto&     \left( \langle a_i,u\rangle \right)_{i=1}^m
\end{array} \]

\[b=\left( b_i \right)_{i=1}^m \]
\[\{\langle a_i,u\rangle=b_i,\ i=1,...,m\} = \{u; Au=b\}\]

On note $(a,b,c)$ les contraintes de manière générique.

\Lem{}{Pour $u\in\mathcal{U}_{ad}$, on définit l'ensemble $\Lambda(u)$ par :
	\[\Lambda(u)=\{\lambda\in\mathbb{R}^p; \lambda_i\geq 0, \lambda_i c_i(u)=0\ \forall j=1,...,p\}\]
On définit le cône :
	\[\mathscr{N}_{(a,b,c)}(u)=\{A^*\mu+\sum_{j=1}^p \lambda_j s_j,\ \mu\in\mathbb{R}^n,\ \lambda\in\Lambda(u),\ s_j\in\partial c_j(u)\}\]
avec $A^*\mu=\sum_{i=1}^m \mu_i a_i$\\
Alors
	\[\mathscr{N}_{(a,b,c)}(u)\subset \mathscr{N}_{\mathcal{U}_{ad}}(u)\]
}

\Theo{}{Soit $J:\mathbb{R}^n\to\mathbb{R}$ une fonction convexe, et $\bar{u}\in\mathcal{U}_{ad}$. 
\begin{enumerate}
	\item $\bar{u}$ minimise $J$ sur $\mathcal{U}_{ad}$
	\item $0\in\partial J(\bar{u}) + \mathcal{N}_{\bar{u}}\mathcal{U}_{ad}$.
	\item $\exists \mu\in\mathbb{R}^n$, $\exists \lambda\in\Lambda(\bar{u})$; $0\in \partial J(\bar{u}) + \sum_{i=1}^m a_i\mu_i + \sum_{j=1}^p \lambda_j\partial c_j(\bar{u})$
\end{enumerate}
Alors $(3)\Rightarrow (1) \Leftrightarrow (2)$}

\subsubsection{Qualification des contraintes}
Les contraintes sont qualifiées si :
	\[\mathscr{N}_{(a,b,c)}(u)=\mathscr{N}_{\mathcal{U}_{ad}}(u) \text{ et alors } (3)\Leftrightarrow(1)\]

\Lem{}{Si les contraintes $c_j$ sont affines et si $\mathcal{U}_{ad}$ est non vide, alors 
	\[\mathscr{N}_{(a,b,c)}(u)= \mathscr{N}_{\mathcal{U}_{ad}}(u)\]
}

\Lem{}{Si $s_1,...,s_m\in\mathbb{R}^n$, alors $cone(s_1,...,s_m)$ est fermé.}

On oublie le cas affine :
\Theo{}{On fait l'hypothèse dite de Slatter : 
\[\exists u_0;\ \left\{ \begin{array}{l r}
	Au_0=b\\
	c_j(u_0)<0 & \forall j=1,...,p
\end{array}\right.\]
$\bar{u}$ minimise $J$ sur $\mathcal{U}_{ad}$ si et seulement si :
	\[\exists \mu\in\mathbb{R}^n, \exists \lambda\in\Lambda(\bar{u}); 0\in \partial J(\bar{u}) + \sum_{i=1}^m a_i\mu_i + \sum_{j=1}^p \lambda_j\partial c_j(\bar{u})\]
}
