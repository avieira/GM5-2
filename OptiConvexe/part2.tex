\part{Conditions d'optimalité}
\[\min_{u\in\mathcal{U}_{ad}} J(u)\]
$\mathcal{U}_{ad}\subset\mathbb{R}^n$ est l'ensemble (non vide) admissible.
On suppose $\mathcal{U}_{ad}$ fermé convexe, et $J$ convexe.

\Theo{}{Si $J$ est coercive ou si $\mathcal{U}_{ad}$ est borné, alors il existe un point de minimum.}

\section{Une condition nécessaire générale d'optimalité}
\Def{Cône tangent}{On dit que $d\in\mathbb{R}^n$ est une tangente à $X$ en $\bar{x}$ si $\exists x_k\to\bar{x}$ avec $(x_k\subset X$, $t_k\to 0$, $t_k>0$ tel que :
	\[\frac{x_k-\bar{x}}{t_k}\to d\]
L'ensemble de toutes les directions tangentes est appelé le cône tangent et est noté $T_{\bar{x}}X$.}

\Def{équivalente}{$d\in T_{\bar{x}}X$ si $\exists t_k>0$, $t_k\to 0$ et $\exists d_k\in X$, $d_k\to d$ tel que $\bar{x}+t_kd_k\in X$.}

\Propo{}{$T_{\bar{x}}X$ est un cône fermé. Il est convexe si $X$ est convexe.}

\Propo{}{Soient $X$ un ensemble convexe et $\bar{x}\in X$. Alors \[T_{\bar{x}}X=\overline{cone}(X-\bar{x})=\overline{\mathbb{R}_+(X-\bar{x})}\]}

\Def{}{Soient $X\subset\mathbb{R}^n$, $\bar{x}\in X$.\\
On dit que $p$ est une direction normale à $X$ en $\bar{x}$ si \[\langle p,d\rangle\leq 0\ \forall d\in T_{\bar{x}}X\]
L'ensemble des normales est appelé le cône normal, noté $\mathcal{N}_{\bar{x}}X$.}

\Rem{}{$\mathcal{N}_{\bar{x}}X=\left(T_{\bar{x}}X\right)^-=-\left(T_{\bar{x}}X\right)^*$\\
$\mathcal{N}_{\bar{x}}X$ est donc un cône convexe.}

\Theo{}{Soit $\mathcal{U}_{ad}$ un ensemble convexe fermé non vide, $J:\mathbb{R}^n\to\mathbb{R}$ une fonction convexe, et $\bar{u}\in\mathcal{U}_{ad}$. Les assertions suivantes sont équivalentes :
\begin{enumerate}
	\item $\bar{u}$ minimise $J$ sur $\mathcal{U}_{ad}$
	\item $J'(\bar{u},u-\bar{u})\geq 0$ $\forall u\in \mathcal{U}_{ad}$
	\item $J'(\bar{u},d)\geq 0$ $\forall d\in T_{\bar{u}}\mathcal{U}_{ad}$
	\item $0\in\partial J(\bar{u}) + \mathcal{N}_{\bar{u}}\mathcal{U}_{ad}$.
\end{enumerate}}

\section{Cas où les contraintes sont explicites}
\[\mathcal{U}_{ad}=\left\{\begin{array}{c l r}
	u; & \langle a_i, u\rangle = b_i & i=1,...,m\\
	   & c_j(u)\leq 0 		 & j=1,...,p
\end{array}\right\} \]

$a_i\in\mathbb{R}^n$, $b_i\in\mathbb{R}$, $c_j:\mathbb{R}^n\to\mathbb{R}$ convexe.\\
$\mathcal{U}_{ad}$ est convexe.\\
On note \[\begin{array}{l c c c}
A : & \mathbb{R}^n &\to& \mathbb{R}^m\\
    &       u    &\mapsto&     \left( \langle a_i,u\rangle \right)_{i=1}^m
\end{array} \]

\[b=\left( b_i \right)_{i=1}^m \]
\[\{\langle a_i,u\rangle=b_i,\ i=1,...,m\} = \{u; Au=b\}\]

On note $(a,b,c)$ les contraintes de manière générique.

\Lem{}{Pour $u\in\mathcal{U}_{ad}$, on définit l'ensemble $\Lambda(u)$ par :
	\[\Lambda(u)=\{\lambda\in\mathbb{R}^p; \lambda_i\geq 0, \lambda_i c_i(u)=0\ \forall j=1,...,p\}\]
On définit le cône :
	\[\mathscr{N}_{(a,b,c)}(u)=\{A^*\mu+\sum_{j=1}^p \lambda_j s_j,\ \mu\in\mathbb{R}^n,\ \lambda\in\Lambda(u),\ s_j\in\partial c_j(u)\}\]
avec $A^*\mu=\sum_{i=1}^m \mu_i a_i$\\
Alors
	\[\mathscr{N}_{(a,b,c)}(u)\subset \mathscr{N}_{\mathcal{U}_{ad}}(u)\]
}

\Theo{}{Soit $J:\mathbb{R}^n\to\mathbb{R}$ une fonction convexe, et $\bar{u}\in\mathcal{U}_{ad}$. 
\begin{enumerate}
	\item $\bar{u}$ minimise $J$ sur $\mathcal{U}_{ad}$
	\item $0\in\partial J(\bar{u}) + \mathcal{N}_{\bar{u}}\mathcal{U}_{ad}$.
	\item $\exists \mu\in\mathbb{R}^n$, $\exists \lambda\in\Lambda(\bar{u})$; $0\in \partial J(\bar{u}) + \sum_{i=1}^m a_i\mu_i + \sum_{j=1}^p \lambda_j\partial c_j(\bar{u})$
\end{enumerate}
Alors $(3)\Rightarrow (1) \Leftrightarrow (2)$}

\subsection{Qualification des contraintes}
Les contraintes sont qualifiées si :
	\[\mathscr{N}_{(a,b,c)}(u)=\mathscr{N}_{\mathcal{U}_{ad}}(u) \text{ et alors } (3)\Leftrightarrow(1)\]

\Lem{}{Si les contraintes $c_j$ sont affines et si $\mathcal{U}_{ad}$ est non vide, alors 
	\[\mathscr{N}_{(a,b,c)}(u)= \mathscr{N}_{\mathcal{U}_{ad}}(u)\]
}

\Lem{}{Si $s_1,...,s_m\in\mathbb{R}^n$, alors $cone(s_1,...,s_m)$ est fermé.}

On oublie le cas affine :
\Theo{}{On fait l'hypothèse dite de Slatter : 
\[\exists u_0;\ \left\{ \begin{array}{l r}
	Au_0=b\\
	c_j(u_0)<0 & \forall j=1,...,p
\end{array}\right.\]
$\bar{u}$ minimise $J$ sur $\mathcal{U}_{ad}$ si et seulement si :
	\[\exists \mu\in\mathbb{R}^n, \exists \lambda\in\Lambda(\bar{u}); 0\in \partial J(\bar{u}) + \sum_{i=1}^m a_i\mu_i + \sum_{j=1}^p \lambda_j\partial c_j(\bar{u})\]
}
